{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "oxiZ42B4SwQ-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchsummaryX import summary\n",
    "from tests_hw4 import test_prediction, test_generation\n",
    "from tqdm import tqdm\n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec  3 04:24:32 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   56C    P0    29W /  70W |  12399MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     36170      C   ...a/envs/pytorch/bin/python    12395MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 30\n",
    "SEQ_LEN = 50\n",
    "EMB_DIM = 200\n",
    "HIDDEN_SIZE = 200\n",
    "LR = 0.001\n",
    "SEQ_LEN_PROB = 0.95\n",
    "SEQ_LEN_STD = 5\n",
    "LSTM_DROPOUT = 0\n",
    "LSTM_LAYERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "x5znxQhLSwRC"
   },
   "outputs": [],
   "source": [
    "# load all that we need\n",
    "\n",
    "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
    "devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)\n",
    "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
    "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
    "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
    "vocab = np.load('../dataset/vocab.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "OZNrJ8XvSwRF"
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class DataLoaderForLanguageModeling(DataLoader):\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, seq_len, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.shuffle = shuffle\n",
    "        self.num_batches = (len(np.concatenate(dataset))-1)//batch_size\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "      return int(self.num_batches // self.seq_len)\n",
    "      \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "            You may implement some of the techniques in https://arxiv.org/pdf/1708.02182.pdf\n",
    "            example: Variable length backpropagation sequences (Section 4.1)\n",
    "        \"\"\"\n",
    "        ## dataset = Array of articles; article = array of ints\n",
    "        # 1. Randomly shuffle all the articles from the WikiText-2 dataset.\n",
    "        if(self.shuffle):\n",
    "            np.random.shuffle(self.dataset)\n",
    "        # 2. Concatenate all text in one long string.\n",
    "        data = np.concatenate(self.dataset)\n",
    "        # 3. Group the sequences into batches.\n",
    "        self.num_batches = (len(data) - 1) // self.batch_size # One less since need offset for label\n",
    "        inputs  = data[0:self.num_batches * self.batch_size].reshape(self.batch_size,-1)\n",
    "        targets = data[1:self.num_batches * self.batch_size + 1].reshape(self.batch_size,-1)\n",
    "        inputs = torch.from_numpy(inputs).to(dtype=torch.long)\n",
    "        targets = torch.from_numpy(targets).to(dtype=torch.long)\n",
    "        # 4. Run a loop that returns a tuple of (input, label) on every iteration with yield.\n",
    "        offset = 0\n",
    "        cur_seq_len = self.seq_len\n",
    "        while((offset + cur_seq_len) < inputs.shape[1]):\n",
    "            input_ = inputs[:, offset : offset+cur_seq_len]\n",
    "            target = targets[:, offset : offset+cur_seq_len]\n",
    "            offset += cur_seq_len\n",
    "            yield (input_, target)\n",
    "            # Update cur_seq_len\n",
    "            cur_seq_len = self.seq_len if (np.random.rand() < SEQ_LEN_PROB) else self.seq_len//2\n",
    "            cur_seq_len_temp = int(np.random.normal(cur_seq_len, SEQ_LEN_STD))\n",
    "            cur_seq_len = cur_seq_len_temp if (cur_seq_len_temp > 0) else cur_seq_len\n",
    "\n",
    "# # TEST       \n",
    "# test = DataLoaderForLanguageModeling(dataset, BATCH_SIZE, SEQ_LEN)\n",
    "# for i,(test_inputs, test_targets) in enumerate(test.__iter__()):\n",
    "#     print('---------')\n",
    "#     print('iter: ', i)\n",
    "#     print('shape: ', test_inputs.shape)\n",
    "#     print('type: ', test_inputs.dtype, ', ', test_targets.dtype)\n",
    "#     for batch_idx in range(0, test.batch_size):\n",
    "#         tmpstr1 = ['    ']\n",
    "#         tmpstr2 = ['    ']\n",
    "#         for seq_idx in range(0, test.seq_len):\n",
    "#             tmpstr1.append(vocab[test_inputs[batch_idx, seq_idx]])\n",
    "#             tmpstr2.append(vocab[test_targets[batch_idx, seq_idx]])\n",
    "#         print(' '.join(tmpstr1))\n",
    "#         print(' '.join(tmpstr2))\n",
    "#         print()\n",
    "#     if(i > 3):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "Zt-7YsTYSwRI"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "        TODO: Define your model here\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int, hidden_size:int):\n",
    "        super(Model, self).__init__()\n",
    "        # Embedding: vocab_size -> embedding_dim\n",
    "        # LSTM: embedding_dim -> hidden_size\n",
    "        # Classifier: hidden_size -> vocab_size\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_dim),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embedding_dim,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = LSTM_LAYERS,\n",
    "            # bidirectional = True,\n",
    "            dropout = LSTM_DROPOUT,\n",
    "            batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * 1, vocab_size),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, h_in = None):\n",
    "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
    "        out = self.embedding(x)\n",
    "        out, h_out = self.lstm(out, h_in) if h_in else self.lstm(out)\n",
    "        out = self.classifier(out)\n",
    "        return out, h_out\n",
    "\n",
    "# # TEST\n",
    "# model = Model(len(vocab), EMB_DIM, HIDDEN_SIZE)\n",
    "# test_input = torch.randint(0, len(vocab), (BATCH_SIZE, SEQ_LEN), dtype=torch.long)\n",
    "# test_output, test_hidden = model(test_input)\n",
    "# print('Input : ', test_input.shape, ', ', test_input.dtype)\n",
    "# print('Output: ', test_output.shape, ', ', test_output.dtype) # (batch, seq_len, vocab_size)\n",
    "# # summary(model, test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "kIvZOIfjSwRK"
   },
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        # feel free to define a learning rate scheduler as well if you want\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "        self.criterion = nn.CrossEntropyLoss() # Correct???\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        batch_bar = tqdm(total=len(self.loader), dynamic_ncols=True, leave=False, position=0, desc='Batch')\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "            batch_bar.update()\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "            \n",
    "            :return \n",
    "                    (float) loss value\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        # Forwards\n",
    "        inputs = inputs.to(device)\n",
    "        outputs, _ = self.model(inputs)\n",
    "        # Compute loss\n",
    "        targets = targets.to(device)\n",
    "        loss = self.criterion(\n",
    "            outputs.reshape(-1, outputs.shape[2]), # (instances, classes)\n",
    "            targets.reshape(-1) # (instances, )\n",
    "        )\n",
    "        # Backwards\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval() # set to eval mode\n",
    "        predictions = TestLanguageModel.predict(fixtures_pred['inp'], self.model) # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "        generated_logits = TestLanguageModel.generate(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
    "        generated_logits_test = TestLanguageModel.generate(fixtures_gen_test, 10, self.model)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "        \n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.predict(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "            \n",
    "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, nll))\n",
    "        self.epochs += 1\n",
    "\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "            model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "xPI7_kZRSwRN"
   },
   "outputs": [],
   "source": [
    "class TestLanguageModel:\n",
    "    def predict(inp, model):\n",
    "        \"\"\"\n",
    "            TODO: write prediction code here\n",
    "            \n",
    "            :param inp:\n",
    "            :return: a np.ndarray of logits\n",
    "        \"\"\"\n",
    "        outputs, _ = model(torch.Tensor(inp).long().to(device))\n",
    "        # predictions = torch.argmax(outputs[:,-1,:], dim=1).unsqueeze(0)\n",
    "        predictions = outputs[:,-1,:]\n",
    "        return predictions.cpu().detach().numpy()\n",
    "\n",
    "        \n",
    "    def generate(inp, forward, model):\n",
    "        \"\"\"\n",
    "            TODO: write generation code here\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"        \n",
    "        new_words = []\n",
    "        hidden = None\n",
    "        cur_inp = torch.clone(torch.Tensor(inp).long())\n",
    "        for i in range(0, forward):\n",
    "          out, hidden = model(cur_inp.to(device), hidden)\n",
    "          cur_new_words = torch.argmax(out, dim=2)[:,-1] # Only grab last word per sequence for each batch\n",
    "          new_words.append(cur_new_words)\n",
    "          cur_inp = torch.unsqueeze(cur_new_words, dim=1) # (batch,) -> (batch,seq)\n",
    "        new_words = torch.stack(new_words, dim=1) # (batch, forward)\n",
    "        return new_words.cpu().detach().numpy()\n",
    "\n",
    "# # TEST\n",
    "# test_input = torch.randint(0, len(vocab), (BATCH_SIZE, SEQ_LEN))\n",
    "# test_model = Model(len(vocab), EMB_DIM, HIDDEN_SIZE)\n",
    "# test_output = TestLanguageModel.predict(test_input, test_model)\n",
    "# print('Test predict : ', test_output.shape)\n",
    "# test_output = TestLanguageModel.generate(test_input, 20, test_model)\n",
    "# print('Test generate: ', test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "2HCVG5YISwRW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./experiments/1670048045\n"
     ]
    }
   ],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "DbHH6zXTSwRa"
   },
   "outputs": [],
   "source": [
    "model = Model(len(vocab), embedding_dim=EMB_DIM, hidden_size=HIDDEN_SIZE).to(device)\n",
    "\n",
    "loader = DataLoaderForLanguageModeling(\n",
    "    dataset=dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    seq_len=SEQ_LEN,\n",
    "    shuffle=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    loader=loader, \n",
    "    max_epochs=NUM_EPOCHS, \n",
    "    run_id=run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "7D8wTJkBSwRc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [1/20]   Loss: 7.1889\n",
      "[VAL]  Epoch [1/20]   Loss: 6.7060\n",
      "Epoch:  2 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [2/20]   Loss: 6.7176\n",
      "[VAL]  Epoch [2/20]   Loss: 5.7831\n",
      "Epoch:  3 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [3/20]   Loss: 6.0955\n",
      "[VAL]  Epoch [3/20]   Loss: 5.4538\n",
      "Epoch:  4 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [4/20]   Loss: 5.8145\n",
      "[VAL]  Epoch [4/20]   Loss: 5.2382\n",
      "Saving model, predictions and generated output for epoch 3 with NLL: 5.238165\n",
      "Epoch:  5 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [5/20]   Loss: 5.6274\n",
      "[VAL]  Epoch [5/20]   Loss: 5.1675\n",
      "Saving model, predictions and generated output for epoch 4 with NLL: 5.1674986\n",
      "Epoch:  6 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [6/20]   Loss: 5.4752\n",
      "[VAL]  Epoch [6/20]   Loss: 5.0022\n",
      "Saving model, predictions and generated output for epoch 5 with NLL: 5.002163\n",
      "Epoch:  7 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [7/20]   Loss: 5.3460\n",
      "[VAL]  Epoch [7/20]   Loss: 4.8890\n",
      "Saving model, predictions and generated output for epoch 6 with NLL: 4.888969\n",
      "Epoch:  8 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [8/20]   Loss: 5.2377\n",
      "[VAL]  Epoch [8/20]   Loss: 4.8158\n",
      "Saving model, predictions and generated output for epoch 7 with NLL: 4.815801\n",
      "Epoch:  9 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [9/20]   Loss: 5.1460\n",
      "[VAL]  Epoch [9/20]   Loss: 4.7240\n",
      "Saving model, predictions and generated output for epoch 8 with NLL: 4.7240033\n",
      "Epoch:  10 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [10/20]   Loss: 5.0626\n",
      "[VAL]  Epoch [10/20]   Loss: 4.7322\n",
      "Epoch:  11 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [11/20]   Loss: 4.9909\n",
      "[VAL]  Epoch [11/20]   Loss: 4.7595\n",
      "Epoch:  12 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [12/20]   Loss: 4.9259\n",
      "[VAL]  Epoch [12/20]   Loss: 4.7370\n",
      "Epoch:  13 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [13/20]   Loss: 4.8682\n",
      "[VAL]  Epoch [13/20]   Loss: 4.6835\n",
      "Saving model, predictions and generated output for epoch 12 with NLL: 4.6834526\n",
      "Epoch:  14 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [14/20]   Loss: 4.8137\n",
      "[VAL]  Epoch [14/20]   Loss: 4.7043\n",
      "Epoch:  15 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [15/20]   Loss: 4.7634\n",
      "[VAL]  Epoch [15/20]   Loss: 4.6964\n",
      "Epoch:  16 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [16/20]   Loss: 4.7178\n",
      "[VAL]  Epoch [16/20]   Loss: 4.7405\n",
      "Epoch:  17 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [17/20]   Loss: 4.6754\n",
      "[VAL]  Epoch [17/20]   Loss: 4.6802\n",
      "Saving model, predictions and generated output for epoch 16 with NLL: 4.680168\n",
      "Epoch:  18 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [18/20]   Loss: 4.6342\n",
      "[VAL]  Epoch [18/20]   Loss: 4.7344\n",
      "Epoch:  19 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [19/20]   Loss: 4.6004\n",
      "[VAL]  Epoch [19/20]   Loss: 4.6558\n",
      "Saving model, predictions and generated output for epoch 18 with NLL: 4.6558275\n",
      "Epoch:  20 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [20/20]   Loss: 4.5621\n",
      "[VAL]  Epoch [20/20]   Loss: 4.6693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# best_nll = 1e30 \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Epoch: ', epoch+1, '/', NUM_EPOCHS)\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
    "        trainer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "z2FmDqBCSwRf"
   },
   "outputs": [],
   "source": [
    "# Don't change these\n",
    "# plot training curves\n",
    "# plt.figure()\n",
    "# plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "# plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('NLL')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "ipdbmqaGSwRh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | not in the area , and the <unk> <unk> <unk>\n",
      "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | = = = <eol> The first season of the season\n",
      "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ highest week , and the first time that the\n",
      "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | <unk> , and <unk> . The episode was released in\n",
      "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | amount of <unk> , and the <unk> of the <unk>\n",
      "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | in the United States . <eol> = = = =\n",
      "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | . <eol> = = = = <unk> = = =\n",
      "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | the <unk> . The <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | . The ship was the first to be the first\n",
      "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , the storm was the first to be the first\n",
      "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | a <unk> in the United States in the United States\n",
      "Input | Output #12: — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | to the <unk> . <eol> = = = = <unk>\n",
      "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | the Congo , and the <unk> of the <unk> of\n",
      "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | , he was a <unk> of the <unk> of the\n",
      "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | and <unk> <unk> , who had been the first time\n",
      "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | 's work was not to be the most popular and\n",
      "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | The episode was released in the United States and the\n",
      "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | @,@ 000 t ) of <unk> , and the <unk>\n",
      "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | the storm had been destroyed . <eol> = = =\n",
      "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | in the <unk> , and the <unk> of the <unk>\n",
      "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | 2 @,@ <unk> ft ) . The <unk> <unk> <unk>\n",
      "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | United States , and the <unk> of the <unk> <unk>\n",
      "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | the United States , and the <unk> <unk> <unk> <unk>\n",
      "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | . <eol> = = = = <unk> = = =\n",
      "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | Twenty Highway , which was the first single to be\n",
      "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | were also included in the United States , and the\n",
      "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = <eol> The first game was the first time\n",
      "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | \" . The song was released on the Billboard Hot\n",
      "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | . <eol> = = = = <unk> = = =\n",
      "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | that the game was \" <unk> \" . <eol> =\n",
      "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | . <eol> = = = = <unk> = = =\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see generated output\n",
    "print (trainer.generated[-1]) # get last generated output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "49babd242887c40fb16c5691cced875369a9ddf7aecdf16cfe0450dd8e53e3e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
