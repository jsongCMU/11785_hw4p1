{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oxiZ42B4SwQ-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2292"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchsummaryX import summary\n",
    "from tests_hw4 import test_prediction, test_generation\n",
    "from tqdm import tqdm\n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec  2 23:58:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   37C    P0    27W /  70W |    951MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     26182      C   ...a/envs/pytorch/bin/python      947MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 5\n",
    "SEQ_LEN = 50\n",
    "EMB_DIM = 10\n",
    "HIDDEN_SIZE = 10\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x5znxQhLSwRC"
   },
   "outputs": [],
   "source": [
    "# load all that we need\n",
    "\n",
    "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
    "devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)\n",
    "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
    "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
    "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
    "vocab = np.load('../dataset/vocab.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "OZNrJ8XvSwRF"
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class DataLoaderForLanguageModeling(DataLoader):\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, seq_len, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.shuffle = shuffle\n",
    "        self.num_batches = (len(np.concatenate(dataset))-1)//batch_size\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "      return int(self.num_batches // self.seq_len)\n",
    "      \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "            You may implement some of the techniques in https://arxiv.org/pdf/1708.02182.pdf\n",
    "            example: Variable length backpropagation sequences (Section 4.1)\n",
    "        \"\"\"\n",
    "        ## dataset = Array of articles; article = array of ints\n",
    "        # 1. Randomly shuffle all the articles from the WikiText-2 dataset.\n",
    "        if(self.shuffle):\n",
    "            np.random.shuffle(self.dataset)\n",
    "        # 2. Concatenate all text in one long string.\n",
    "        data = np.concatenate(self.dataset)\n",
    "        # 3. Group the sequences into batches.\n",
    "        self.num_batches = (len(data) - 1) // self.batch_size # One less since need offset for label\n",
    "        inputs  = data[0:self.num_batches * self.batch_size].reshape(self.batch_size,-1)\n",
    "        targets = data[1:self.num_batches * self.batch_size + 1].reshape(self.batch_size,-1)\n",
    "        inputs = torch.from_numpy(inputs).to(dtype=torch.long)\n",
    "        targets = torch.from_numpy(targets).to(dtype=torch.long)\n",
    "        # 4. Run a loop that returns a tuple of (input, label) on every iteration with yield.\n",
    "        offset = 0\n",
    "        while((offset + self.seq_len) < inputs.shape[1]):\n",
    "            input_ = inputs[:, offset : offset+self.seq_len]\n",
    "            target = targets[:, offset : offset+self.seq_len]\n",
    "            offset += self.seq_len\n",
    "            self.temp1 = offset\n",
    "            self.temp2 = offset + self.seq_len\n",
    "            self.temp3 = self.num_batches * self.batch_size\n",
    "            yield (input_, target)\n",
    "\n",
    "# # TEST       \n",
    "# test = DataLoaderForLanguageModeling(dataset, BATCH_SIZE, SEQ_LEN)\n",
    "# for i,(test_inputs, test_targets) in enumerate(test.__iter__()):\n",
    "#     print('---------')\n",
    "#     print('iter: ', i)\n",
    "#     print('shape: ', test_inputs.shape)\n",
    "#     print('type: ', test_inputs.dtype, ', ', test_targets.dtype)\n",
    "#     for batch_idx in range(0, test.batch_size):\n",
    "#         tmpstr1 = ['    ']\n",
    "#         tmpstr2 = ['    ']\n",
    "#         for seq_idx in range(0, test.seq_len):\n",
    "#             tmpstr1.append(vocab[test_inputs[batch_idx, seq_idx]])\n",
    "#             tmpstr2.append(vocab[test_targets[batch_idx, seq_idx]])\n",
    "#         print(' '.join(tmpstr1))\n",
    "#         print(' '.join(tmpstr2))\n",
    "#         print()\n",
    "#     if(i > 3):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "id": "Zt-7YsTYSwRI"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "        TODO: Define your model here\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int, hidden_size:int):\n",
    "        super(Model, self).__init__()\n",
    "        # Embedding: vocab_size -> embedding_dim\n",
    "        # LSTM: embedding_dim -> hidden_size\n",
    "        # Classifier: hidden_size -> vocab_size\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embedding_dim,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = 1,\n",
    "            bidirectional = True,\n",
    "            batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * 2, vocab_size),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, h_in = None):\n",
    "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
    "        out = self.embedding(x)\n",
    "        out, h_out = self.lstm(out, h_in) if h_in else self.lstm(out)\n",
    "        out = self.classifier(out)\n",
    "        return out, h_out\n",
    "\n",
    "# # TEST\n",
    "# model = Model(len(vocab), EMB_DIM, HIDDEN_SIZE)\n",
    "# test_input = torch.randint(0, len(vocab), (BATCH_SIZE, SEQ_LEN), dtype=torch.long)\n",
    "# test_output, test_hidden = model(test_input)\n",
    "# print('Input : ', test_input.shape, ', ', test_input.dtype)\n",
    "# print('Output: ', test_output.shape, ', ', test_output.dtype) # (batch, seq_len, vocab_size)\n",
    "# # summary(model, test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "id": "kIvZOIfjSwRK"
   },
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        # feel free to define a learning rate scheduler as well if you want\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "        self.criterion = nn.CrossEntropyLoss() # Correct???\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        batch_bar = tqdm(total=len(self.loader), dynamic_ncols=True, leave=False, position=0, desc='Batch')\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "            batch_bar.update()\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "            \n",
    "            :return \n",
    "                    (float) loss value\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        # Forwards\n",
    "        inputs = inputs.to(device)\n",
    "        outputs, _ = self.model(inputs)\n",
    "        # Compute loss\n",
    "        targets = targets.to(device)\n",
    "        loss = self.criterion(\n",
    "            outputs.reshape(-1, outputs.shape[2]), # (instances, classes)\n",
    "            targets.reshape(-1) # (instances, )\n",
    "        )\n",
    "        # Backwards\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval() # set to eval mode\n",
    "        predictions = TestLanguageModel.predict(fixtures_pred['inp'], self.model) # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "        generated_logits = TestLanguageModel.generate(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
    "        generated_logits_test = TestLanguageModel.generate(fixtures_gen_test, 10, self.model)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "        \n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.predict(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "            \n",
    "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, nll))\n",
    "        self.epochs += 1\n",
    "\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "            model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "xPI7_kZRSwRN"
   },
   "outputs": [],
   "source": [
    "class TestLanguageModel:\n",
    "    def predict(inp, model):\n",
    "        \"\"\"\n",
    "            TODO: write prediction code here\n",
    "            \n",
    "            :param inp:\n",
    "            :return: a np.ndarray of logits\n",
    "        \"\"\"\n",
    "        outputs, _ = model(torch.Tensor(inp).long().to(device))\n",
    "        # predictions = torch.argmax(outputs[:,-1,:], dim=1).unsqueeze(0)\n",
    "        predictions = outputs[:,-1,:]\n",
    "        return predictions.cpu().detach().numpy()\n",
    "\n",
    "        \n",
    "    def generate(inp, forward, model):\n",
    "        \"\"\"\n",
    "            TODO: write generation code here\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"        \n",
    "        new_words = []\n",
    "        hidden = None\n",
    "        cur_inp = torch.clone(torch.Tensor(inp).long())\n",
    "        for i in range(0, forward):\n",
    "          out, hidden = model(cur_inp.to(device), hidden)\n",
    "          cur_new_words = torch.argmax(out, dim=2)[:,-1] # Only grab last word per sequence for each batch\n",
    "          new_words.append(cur_new_words)\n",
    "          cur_inp = torch.unsqueeze(cur_new_words, dim=1) # (batch,) -> (batch,seq)\n",
    "        new_words = torch.stack(new_words, dim=1) # (batch, forward)\n",
    "        return new_words.cpu().detach().numpy()\n",
    "\n",
    "# # TEST\n",
    "# test_input = torch.randint(0, len(vocab), (BATCH_SIZE, SEQ_LEN))\n",
    "# test_model = Model(len(vocab), EMB_DIM, HIDDEN_SIZE)\n",
    "# test_output = TestLanguageModel.predict(test_input, test_model)\n",
    "# print('Test predict : ', test_output.shape)\n",
    "# test_output = TestLanguageModel.generate(test_input, 20, test_model)\n",
    "# print('Test generate: ', test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "id": "2HCVG5YISwRW"
   },
   "outputs": [],
   "source": [
    "# run_id = str(int(time.time()))\n",
    "# if not os.path.exists('./experiments'):\n",
    "#     os.mkdir('./experiments')\n",
    "# os.mkdir('./experiments/%s' % run_id)\n",
    "# print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "id": "DbHH6zXTSwRa"
   },
   "outputs": [],
   "source": [
    "# model = Model(len(vocab), embedding_dim=EMB_DIM, hidden_size=HIDDEN_SIZE).to(device)\n",
    "\n",
    "loader = DataLoaderForLanguageModeling(\n",
    "    dataset=dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    seq_len=SEQ_LEN,\n",
    "    shuffle=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    loader=loader, \n",
    "    max_epochs=NUM_EPOCHS, \n",
    "    run_id=run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "id": "7D8wTJkBSwRc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [1/5]   Loss: 2.8819\n",
      "[VAL]  Epoch [1/5]   Loss: 6.3241\n",
      "Saving model, predictions and generated output for epoch 0 with NLL: 6.32413\n",
      "foo\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "bar\n",
      "Epoch:  2 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [2/5]   Loss: 2.7097\n",
      "[VAL]  Epoch [2/5]   Loss: 6.2898\n",
      "Saving model, predictions and generated output for epoch 1 with NLL: 6.289777\n",
      "foo\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "AAA\n",
      "bar\n",
      "Epoch:  3 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  31%|████████████████████████████████████████▋                                                                                         | 2602/8302 [00:09<00:20, 277.05it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [300], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m'\u001b[39m, epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, NUM_EPOCHS)\n\u001b[0;32m----> 4\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      5\u001b[0m     nll \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtest()\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m nll \u001b[39m<\u001b[39m best_nll:\n",
      "Cell \u001b[0;32mIn [292], line 34\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m batch_bar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader), dynamic_ncols\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m batch_num, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader):\n\u001b[0;32m---> 34\u001b[0m     epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_batch(inputs, targets)\n\u001b[1;32m     35\u001b[0m     batch_bar\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m     36\u001b[0m epoch_loss \u001b[39m=\u001b[39m epoch_loss \u001b[39m/\u001b[39m (batch_num \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn [292], line 59\u001b[0m, in \u001b[0;36mTrainer.train_batch\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     54\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(\n\u001b[1;32m     55\u001b[0m     outputs\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, outputs\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]), \u001b[39m# (instances, classes)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     targets\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (instances, )\u001b[39;00m\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[39m# Backwards\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     61\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_nll = 1e30 \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Epoch: ', epoch+1, '/', NUM_EPOCHS)\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
    "        trainer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2FmDqBCSwRf"
   },
   "outputs": [],
   "source": [
    "# Don't change these\n",
    "# plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipdbmqaGSwRh"
   },
   "outputs": [],
   "source": [
    "# see generated output\n",
    "print (trainer.generated[-1]) # get last generated output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "49babd242887c40fb16c5691cced875369a9ddf7aecdf16cfe0450dd8e53e3e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
