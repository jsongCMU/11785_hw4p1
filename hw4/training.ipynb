{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oxiZ42B4SwQ-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchsummaryX import summary\n",
    "from tests_hw4 import test_prediction, test_generation\n",
    "from tqdm import tqdm\n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  6 20:17:27 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   29C    P8    14W /  70W |      2MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 30\n",
    "SEQ_LEN = 50\n",
    "EMB_DIM = 200\n",
    "HIDDEN_SIZE = 200\n",
    "LR = 0.001\n",
    "SEQ_LEN_PROB = 0.95\n",
    "SEQ_LEN_STD = 5\n",
    "LSTM_DROPOUT = 0.3\n",
    "LSTM_LAYERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x5znxQhLSwRC"
   },
   "outputs": [],
   "source": [
    "# load all that we need\n",
    "\n",
    "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
    "devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)\n",
    "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
    "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
    "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
    "vocab = np.load('../dataset/vocab.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OZNrJ8XvSwRF"
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class DataLoaderForLanguageModeling(DataLoader):\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, seq_len, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.shuffle = shuffle\n",
    "        self.line_length = (len(np.concatenate(dataset))-1)//batch_size\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "      return int(self.line_length // self.seq_len)\n",
    "      \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "            You may implement some of the techniques in https://arxiv.org/pdf/1708.02182.pdf\n",
    "            example: Variable length backpropagation sequences (Section 4.1)\n",
    "        \"\"\"\n",
    "        ## dataset = Array of articles; article = array of ints\n",
    "        # 1. Randomly shuffle all the articles from the WikiText-2 dataset.\n",
    "        if(self.shuffle):\n",
    "            np.random.shuffle(self.dataset)\n",
    "        # 2. Concatenate all text in one long string.\n",
    "        data = np.concatenate(self.dataset)\n",
    "        # 3. Group the sequences into batches.\n",
    "        self.line_length = (len(data) - 1) // self.batch_size # One less since need offset for label\n",
    "        inputs  = data[0:self.line_length * self.batch_size].reshape(self.batch_size,-1)\n",
    "        targets = data[1:self.line_length * self.batch_size + 1].reshape(self.batch_size,-1)\n",
    "        inputs = torch.from_numpy(inputs).to(dtype=torch.long)\n",
    "        targets = torch.from_numpy(targets).to(dtype=torch.long)\n",
    "        # 4. Run a loop that returns a tuple of (input, label) on every iteration with yield.\n",
    "        offset = 0\n",
    "        cur_seq_len = self.seq_len\n",
    "        while((offset + cur_seq_len) < inputs.shape[1]):\n",
    "            input_ = inputs[:, offset : offset+cur_seq_len]\n",
    "            target = targets[:, offset : offset+cur_seq_len]\n",
    "            offset += cur_seq_len\n",
    "            yield (input_, target)\n",
    "            # Update cur_seq_len\n",
    "            cur_seq_len = self.seq_len if (np.random.rand() < SEQ_LEN_PROB) else self.seq_len//2\n",
    "            cur_seq_len_temp = int(np.random.normal(cur_seq_len, SEQ_LEN_STD))\n",
    "            cur_seq_len = cur_seq_len_temp if (cur_seq_len_temp > 0) else cur_seq_len\n",
    "\n",
    "# # TEST       \n",
    "# test = DataLoaderForLanguageModeling(dataset, BATCH_SIZE, SEQ_LEN)\n",
    "# for i,(test_inputs, test_targets) in enumerate(test.__iter__()):\n",
    "#     print('---------')\n",
    "#     print('iter: ', i)\n",
    "#     print('shape: ', test_inputs.shape)\n",
    "#     print('type: ', test_inputs.dtype, ', ', test_targets.dtype)\n",
    "#     for batch_idx in range(0, test.batch_size):\n",
    "#         tmpstr1 = ['    ']\n",
    "#         tmpstr2 = ['    ']\n",
    "#         for seq_idx in range(0, test.seq_len):\n",
    "#             tmpstr1.append(vocab[test_inputs[batch_idx, seq_idx]])\n",
    "#             tmpstr2.append(vocab[test_targets[batch_idx, seq_idx]])\n",
    "#         print(' '.join(tmpstr1))\n",
    "#         print(' '.join(tmpstr2))\n",
    "#         print()\n",
    "#     if(i > 3):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Zt-7YsTYSwRI"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "        TODO: Define your model here\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int, hidden_size:int):\n",
    "        super(Model, self).__init__()\n",
    "        # Embedding: vocab_size -> embedding_dim\n",
    "        # LSTM: embedding_dim -> hidden_size\n",
    "        # Classifier: hidden_size -> vocab_size\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_dim),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embedding_dim,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = LSTM_LAYERS,\n",
    "            # bidirectional = True,\n",
    "            dropout = LSTM_DROPOUT,\n",
    "            batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * 1, vocab_size),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, h_in = None):\n",
    "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
    "        out = self.embedding(x)\n",
    "        out, h_out = self.lstm(out, h_in) if h_in else self.lstm(out)\n",
    "        out = self.classifier(out)\n",
    "        return out, h_out\n",
    "\n",
    "# # TEST\n",
    "# model = Model(len(vocab), EMB_DIM, HIDDEN_SIZE)\n",
    "# test_input = torch.randint(0, len(vocab), (BATCH_SIZE, SEQ_LEN), dtype=torch.long)\n",
    "# test_output, test_hidden = model(test_input)\n",
    "# print('Input : ', test_input.shape, ', ', test_input.dtype)\n",
    "# print('Output: ', test_output.shape, ', ', test_output.dtype) # (batch, seq_len, vocab_size)\n",
    "# # summary(model, test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kIvZOIfjSwRK"
   },
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        # feel free to define a learning rate scheduler as well if you want\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "        self.criterion = nn.CrossEntropyLoss() # Correct???\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        batch_bar = tqdm(total=len(self.loader), dynamic_ncols=True, leave=False, position=0, desc='Batch')\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "            batch_bar.update()\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "            \n",
    "            :return \n",
    "                    (float) loss value\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        # Forwards\n",
    "        inputs = inputs.to(device)\n",
    "        outputs, _ = self.model(inputs)\n",
    "        # Compute loss\n",
    "        targets = targets.to(device)\n",
    "        loss = self.criterion(\n",
    "            outputs.reshape(-1, outputs.shape[2]), # (instances, classes)\n",
    "            targets.reshape(-1) # (instances, )\n",
    "        )\n",
    "        # Backwards\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval() # set to eval mode\n",
    "        predictions = TestLanguageModel.predict(fixtures_pred['inp'], self.model) # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "        generated_logits = TestLanguageModel.generate(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
    "        generated_logits_test = TestLanguageModel.generate(fixtures_gen_test, 10, self.model)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "        \n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.predict(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "            \n",
    "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, nll))\n",
    "        self.epochs += 1\n",
    "\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "            model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xPI7_kZRSwRN"
   },
   "outputs": [],
   "source": [
    "class TestLanguageModel:\n",
    "    def predict(inp, model):\n",
    "        \"\"\"\n",
    "            TODO: write prediction code here\n",
    "            \n",
    "            :param inp:\n",
    "            :return: a np.ndarray of logits\n",
    "        \"\"\"\n",
    "        outputs, _ = model(torch.Tensor(inp).long().to(device))\n",
    "        # predictions = torch.argmax(outputs[:,-1,:], dim=1).unsqueeze(0)\n",
    "        predictions = outputs[:,-1,:]\n",
    "        return predictions.cpu().detach().numpy()\n",
    "\n",
    "        \n",
    "    def generate(inp, forward, model):\n",
    "        \"\"\"\n",
    "            TODO: write generation code here\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"        \n",
    "        new_words = []\n",
    "        hidden = None\n",
    "        cur_inp = torch.clone(torch.Tensor(inp).long())\n",
    "        for i in range(0, forward):\n",
    "          out, hidden = model(cur_inp.to(device), hidden)\n",
    "          cur_new_words = torch.argmax(out, dim=2)[:,-1] # Only grab last word per sequence for each batch\n",
    "          new_words.append(cur_new_words)\n",
    "          cur_inp = torch.unsqueeze(cur_new_words, dim=1) # (batch,) -> (batch,seq)\n",
    "        new_words = torch.stack(new_words, dim=1) # (batch, forward)\n",
    "        return new_words.cpu().detach().numpy()\n",
    "\n",
    "# # TEST\n",
    "# test_input = torch.randint(0, len(vocab), (BATCH_SIZE, SEQ_LEN))\n",
    "# test_model = Model(len(vocab), EMB_DIM, HIDDEN_SIZE)\n",
    "# test_output = TestLanguageModel.predict(test_input, test_model)\n",
    "# print('Test predict : ', test_output.shape)\n",
    "# test_output = TestLanguageModel.generate(test_input, 20, test_model)\n",
    "# print('Test generate: ', test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2HCVG5YISwRW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./experiments/1670357849\n"
     ]
    }
   ],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DbHH6zXTSwRa"
   },
   "outputs": [],
   "source": [
    "model = Model(len(vocab), embedding_dim=EMB_DIM, hidden_size=HIDDEN_SIZE).to(device)\n",
    "\n",
    "loader = DataLoaderForLanguageModeling(\n",
    "    dataset=dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    seq_len=SEQ_LEN,\n",
    "    shuffle=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    loader=loader, \n",
    "    max_epochs=NUM_EPOCHS, \n",
    "    run_id=run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7D8wTJkBSwRc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_nll:  1e+30\n",
      "Epoch:  1 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [1/100]   Loss: 7.0354\n",
      "[VAL]  Epoch [1/100]   Loss: 6.2320\n",
      "Saving model, predictions and generated output for epoch 0 with NLL: 6.232029\n",
      "Epoch:  2 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [2/100]   Loss: 6.3510\n",
      "[VAL]  Epoch [2/100]   Loss: 5.6328\n",
      "Saving model, predictions and generated output for epoch 1 with NLL: 5.632849\n",
      "Epoch:  3 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [3/100]   Loss: 5.9425\n",
      "[VAL]  Epoch [3/100]   Loss: 5.3246\n",
      "Saving model, predictions and generated output for epoch 2 with NLL: 5.324586\n",
      "Epoch:  4 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [4/100]   Loss: 5.6984\n",
      "[VAL]  Epoch [4/100]   Loss: 5.1700\n",
      "Saving model, predictions and generated output for epoch 3 with NLL: 5.170023\n",
      "Epoch:  5 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [5/100]   Loss: 5.5183\n",
      "[VAL]  Epoch [5/100]   Loss: 4.9878\n",
      "Saving model, predictions and generated output for epoch 4 with NLL: 4.987799\n",
      "Epoch:  6 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [6/100]   Loss: 5.3717\n",
      "[VAL]  Epoch [6/100]   Loss: 4.9285\n",
      "Saving model, predictions and generated output for epoch 5 with NLL: 4.9284563\n",
      "Epoch:  7 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [7/100]   Loss: 5.2359\n",
      "[VAL]  Epoch [7/100]   Loss: 4.8888\n",
      "Saving model, predictions and generated output for epoch 6 with NLL: 4.8888474\n",
      "Epoch:  8 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [8/100]   Loss: 5.1207\n",
      "[VAL]  Epoch [8/100]   Loss: 4.8040\n",
      "Saving model, predictions and generated output for epoch 7 with NLL: 4.803983\n",
      "Epoch:  9 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [9/100]   Loss: 5.0249\n",
      "[VAL]  Epoch [9/100]   Loss: 4.7744\n",
      "Saving model, predictions and generated output for epoch 8 with NLL: 4.774352\n",
      "Epoch:  10 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [10/100]   Loss: 4.9415\n",
      "[VAL]  Epoch [10/100]   Loss: 4.7632\n",
      "Saving model, predictions and generated output for epoch 9 with NLL: 4.7632017\n",
      "Epoch:  11 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [11/100]   Loss: 4.8660\n",
      "[VAL]  Epoch [11/100]   Loss: 4.7746\n",
      "Epoch:  12 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [12/100]   Loss: 4.8011\n",
      "[VAL]  Epoch [12/100]   Loss: 4.7497\n",
      "Saving model, predictions and generated output for epoch 11 with NLL: 4.749675\n",
      "Epoch:  13 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [13/100]   Loss: 4.7408\n",
      "[VAL]  Epoch [13/100]   Loss: 4.7847\n",
      "Epoch:  14 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [14/100]   Loss: 4.6900\n",
      "[VAL]  Epoch [14/100]   Loss: 4.7826\n",
      "Epoch:  15 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [15/100]   Loss: 4.6433\n",
      "[VAL]  Epoch [15/100]   Loss: 4.8033\n",
      "Epoch:  16 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [16/100]   Loss: 4.5937\n",
      "[VAL]  Epoch [16/100]   Loss: 4.7671\n",
      "Epoch:  17 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [17/100]   Loss: 4.5551\n",
      "[VAL]  Epoch [17/100]   Loss: 4.7965\n",
      "Epoch:  18 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [18/100]   Loss: 4.5152\n",
      "[VAL]  Epoch [18/100]   Loss: 4.8098\n",
      "Epoch:  19 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [19/100]   Loss: 4.4784\n",
      "[VAL]  Epoch [19/100]   Loss: 4.7828\n",
      "Epoch:  20 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [20/100]   Loss: 4.4467\n",
      "[VAL]  Epoch [20/100]   Loss: 4.8650\n",
      "Epoch:  21 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [21/100]   Loss: 4.4141\n",
      "[VAL]  Epoch [21/100]   Loss: 4.8074\n",
      "Epoch:  22 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [22/100]   Loss: 4.3838\n",
      "[VAL]  Epoch [22/100]   Loss: 4.8472\n",
      "Epoch:  23 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [23/100]   Loss: 4.3525\n",
      "[VAL]  Epoch [23/100]   Loss: 4.8219\n",
      "Epoch:  24 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [24/100]   Loss: 4.3291\n",
      "[VAL]  Epoch [24/100]   Loss: 4.8394\n",
      "Epoch:  25 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [25/100]   Loss: 4.3004\n",
      "[VAL]  Epoch [25/100]   Loss: 4.8580\n",
      "Epoch:  26 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [26/100]   Loss: 4.2759\n",
      "[VAL]  Epoch [26/100]   Loss: 4.9104\n",
      "Epoch:  27 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [27/100]   Loss: 4.2526\n",
      "[VAL]  Epoch [27/100]   Loss: 4.9005\n",
      "Epoch:  28 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [28/100]   Loss: 4.2297\n",
      "[VAL]  Epoch [28/100]   Loss: 4.8784\n",
      "Epoch:  29 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [29/100]   Loss: 4.2080\n",
      "[VAL]  Epoch [29/100]   Loss: 4.8557\n",
      "Epoch:  30 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [30/100]   Loss: 4.1843\n",
      "[VAL]  Epoch [30/100]   Loss: 4.9307\n",
      "Epoch:  31 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [31/100]   Loss: 4.1643\n",
      "[VAL]  Epoch [31/100]   Loss: 4.9093\n",
      "Epoch:  32 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [32/100]   Loss: 4.1439\n",
      "[VAL]  Epoch [32/100]   Loss: 4.9711\n",
      "Epoch:  33 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [33/100]   Loss: 4.1235\n",
      "[VAL]  Epoch [33/100]   Loss: 4.9305\n",
      "Epoch:  34 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [34/100]   Loss: 4.1081\n",
      "[VAL]  Epoch [34/100]   Loss: 4.9537\n",
      "Epoch:  35 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [35/100]   Loss: 4.0867\n",
      "[VAL]  Epoch [35/100]   Loss: 4.9060\n",
      "Epoch:  36 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Epoch [36/100]   Loss: 4.0714\n",
      "[VAL]  Epoch [36/100]   Loss: 4.9654\n",
      "Epoch:  37 / 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch:  22%|█████████████▌                                                 | 298/1383 [00:10<00:38, 27.86it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m'\u001b[39m, epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, NUM_EPOCHS)\n\u001b[0;32m----> 5\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      6\u001b[0m     nll \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtest()\n\u001b[1;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m nll \u001b[39m<\u001b[39m best_nll:\n",
      "Cell \u001b[0;32mIn [7], line 34\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m batch_bar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader), dynamic_ncols\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m batch_num, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader):\n\u001b[0;32m---> 34\u001b[0m     epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_batch(inputs, targets)\n\u001b[1;32m     35\u001b[0m     batch_bar\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m     36\u001b[0m epoch_loss \u001b[39m=\u001b[39m epoch_loss \u001b[39m/\u001b[39m (batch_num \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn [7], line 50\u001b[0m, in \u001b[0;36mTrainer.train_batch\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     49\u001b[0m \u001b[39m# Forwards\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     51\u001b[0m outputs, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(inputs)\n\u001b[1;32m     52\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# best_nll = 1e30 \n",
    "print('best_nll: ', best_nll)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Epoch: ', epoch+1, '/', NUM_EPOCHS)\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
    "        trainer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2FmDqBCSwRf"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1670357769102 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "# Don't change these\n",
    "# plot training curves\n",
    "# plt.figure()\n",
    "# plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "# plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('NLL')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipdbmqaGSwRh"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8888/'. Verify the server is running and reachable. (request to http://127.0.0.1:8888/api/kernels?1670357769102 failed, reason: connect ECONNREFUSED 127.0.0.1:8888)."
     ]
    }
   ],
   "source": [
    "# see generated output\n",
    "print (trainer.generated[-1]) # get last generated output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "49babd242887c40fb16c5691cced875369a9ddf7aecdf16cfe0450dd8e53e3e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
