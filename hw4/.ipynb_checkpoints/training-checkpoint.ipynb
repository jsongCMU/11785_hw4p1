{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oxiZ42B4SwQ-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tests_hw4 import test_prediction, test_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define other hyperparameters here\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "SEQ_LEN = 10\n",
    "EMB_DIM = 10\n",
    "HIDDEN_SIZE = 10\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "x5znxQhLSwRC"
   },
   "outputs": [],
   "source": [
    "# load all that we need\n",
    "\n",
    "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
    "devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)\n",
    "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
    "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
    "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
    "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
    "vocab = np.load('../dataset/vocab.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OZNrJ8XvSwRF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "iter:  0\n",
      "shape:  torch.Size([2, 10])\n",
      "     = Fern Hobbs = <eol> Fern Hobbs ( May 8\n",
      "     Fern Hobbs = <eol> Fern Hobbs ( May 8 ,\n",
      "\n",
      "     't anticipate some long <unk> . \" <eol> DuVall revealed\n",
      "     anticipate some long <unk> . \" <eol> DuVall revealed in\n",
      "\n",
      "---------\n",
      "iter:  1\n",
      "shape:  torch.Size([2, 10])\n",
      "     , 1883 – April 10 , 1964 ) was an\n",
      "     1883 – April 10 , 1964 ) was an American\n",
      "\n",
      "     in September 2010 that Alice in Chains had not begun\n",
      "     September 2010 that Alice in Chains had not begun writing\n",
      "\n",
      "---------\n",
      "iter:  2\n",
      "shape:  torch.Size([2, 10])\n",
      "     American attorney in the U.S. state of Oregon , and\n",
      "     attorney in the U.S. state of Oregon , and a\n",
      "\n",
      "     writing their next album yet , but \" there 's\n",
      "     their next album yet , but \" there 's plenty\n",
      "\n",
      "---------\n",
      "iter:  3\n",
      "shape:  torch.Size([2, 10])\n",
      "     a private secretary to Oregon Governor Oswald West . She\n",
      "     private secretary to Oregon Governor Oswald West . She was\n",
      "\n",
      "     plenty of riffs flying around . \" He added ,\n",
      "     of riffs flying around . \" He added , \"\n",
      "\n",
      "---------\n",
      "iter:  4\n",
      "shape:  torch.Size([2, 10])\n",
      "     was noted for her ambition and several accomplishments as a\n",
      "     noted for her ambition and several accomplishments as a young\n",
      "\n",
      "     \" That was the case when we first started back\n",
      "     That was the case when we first started back up\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data loader\n",
    "\n",
    "class DataLoaderForLanguageModeling(DataLoader):\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, seq_len, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "            You may implement some of the techniques in https://arxiv.org/pdf/1708.02182.pdf\n",
    "            example: Variable length backpropagation sequences (Section 4.1)\n",
    "        \"\"\"\n",
    "        ## dataset = Array of articles; article = array of ints\n",
    "        # 1. Randomly shuffle all the articles from the WikiText-2 dataset.\n",
    "        if(self.shuffle):\n",
    "            np.random.shuffle(self.dataset)\n",
    "        # 2. Concatenate all text in one long string.\n",
    "        data = np.concatenate(self.dataset)\n",
    "        # 3. Group the sequences into batches.\n",
    "        num_batches = len(data - 1) // self.batch_size # One less since need offset for label\n",
    "        inputs  = data[0:num_batches * self.batch_size].reshape(self.batch_size,-1)\n",
    "        targets = data[1:num_batches * self.batch_size + 1].reshape(self.batch_size,-1)\n",
    "        inputs = torch.from_numpy(inputs)\n",
    "        targets = torch.from_numpy(targets)\n",
    "        # 4. Run a loop that returns a tuple of (input, label) on every iteration with yield.\n",
    "        offset = 0\n",
    "        while(offset + self.seq_len < num_batches * self.batch_size):\n",
    "            input_ = inputs[:, offset : offset+self.seq_len]\n",
    "            target = targets[:, offset : offset+self.seq_len]\n",
    "            offset += self.seq_len\n",
    "            yield (input_, target)\n",
    "\n",
    "# # TEST       \n",
    "# test = DataLoaderForLanguageModeling(dataset, BATCH_SIZE, SEQ_LEN)\n",
    "# for i,(test_inputs, test_targets) in enumerate(test.__iter__()):\n",
    "#     print('---------')\n",
    "#     print('iter: ', i)\n",
    "#     print('shape: ', test_inputs.shape)\n",
    "#     for batch_idx in range(0, test.batch_size):\n",
    "#         tmpstr1 = ['    ']\n",
    "#         tmpstr2 = ['    ']\n",
    "#         for seq_idx in range(0, test.seq_len):\n",
    "#             tmpstr1.append(vocab[test_inputs[batch_idx, seq_idx]])\n",
    "#             tmpstr2.append(vocab[test_targets[batch_idx, seq_idx]])\n",
    "#         print(' '.join(tmpstr1))\n",
    "#         print(' '.join(tmpstr2))\n",
    "#         print()\n",
    "#     if(i > 3):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Zt-7YsTYSwRI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input :  torch.Size([2, 10])\n",
      "Output:  torch.Size([2, 10, 33278])\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "        TODO: Define your model here\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int, hidden_size:int):\n",
    "        super(Model, self).__init__()\n",
    "        # Embedding: vocab_size -> embedding_dim\n",
    "        # LSTM: embedding_dim -> hidden_size\n",
    "        # Classifier: hidden_size -> vocab_size\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embedding_dim,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = 1,\n",
    "            bidirectional = True,\n",
    "            batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            torch.nn.Linear(hidden_size * 2, vocab_size),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
    "        out = self.embedding(x)\n",
    "        out,_ = self.lstm(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "# # TEST\n",
    "# model = Model(len(vocab), EMB_DIM, HIDDEN_SIZE)\n",
    "# test_input = torch.randint(0, len(vocab), (BATCH_SIZE, SEQ_LEN))\n",
    "# test_output = model(test_input)\n",
    "# print('Input : ', test_input.shape)\n",
    "# print('Output: ', test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIvZOIfjSwRK"
   },
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.predictions = []\n",
    "        self.predictions_test = []\n",
    "        self.generated_logits = []\n",
    "        self.generated = []\n",
    "        self.generated_logits_test = []\n",
    "        self.generated_test = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        # TODO: Define your optimizer and criterion here\n",
    "        # feel free to define a learning rate scheduler as well if you want\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "        self.criterion = nn.CrossEntropyLoss() # Correct???\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
    "            epoch_loss += self.train_batch(inputs, targets)\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        \"\"\" \n",
    "            TODO: Define code for training a single batch of inputs\n",
    "            \n",
    "            :return \n",
    "                    (float) loss value\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "        # don't change these\n",
    "        self.model.eval() # set to eval mode\n",
    "        predictions = TestLanguageModel.predict(fixtures_pred['inp'], self.model) # get predictions\n",
    "        self.predictions.append(predictions)\n",
    "        generated_logits = TestLanguageModel.generate(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
    "        generated_logits_test = TestLanguageModel.generate(fixtures_gen_test, 10, self.model)\n",
    "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
    "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
    "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
    "        self.val_losses.append(nll)\n",
    "        \n",
    "        self.generated.append(generated)\n",
    "        self.generated_test.append(generated_test)\n",
    "        self.generated_logits.append(generated_logits)\n",
    "        self.generated_logits_test.append(generated_logits_test)\n",
    "        \n",
    "        # generate predictions for test data\n",
    "        predictions_test = TestLanguageModel.predict(fixtures_pred_test['inp'], self.model) # get predictions\n",
    "        self.predictions_test.append(predictions_test)\n",
    "            \n",
    "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs + 1, self.max_epochs, nll))\n",
    "        self.epochs += 1\n",
    "\n",
    "        return nll\n",
    "\n",
    "    def save(self):\n",
    "        # don't change these\n",
    "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
    "        torch.save({'state_dict': self.model.state_dict()},\n",
    "            model_path)\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
    "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated[-1])\n",
    "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
    "            fw.write(self.generated_test[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPI7_kZRSwRN"
   },
   "outputs": [],
   "source": [
    "class TestLanguageModel:\n",
    "    def predict(inp, model):\n",
    "        \"\"\"\n",
    "            TODO: write prediction code here\n",
    "            \n",
    "            :param inp:\n",
    "            :return: a np.ndarray of logits\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "        \n",
    "    def generate(inp, forward, model):\n",
    "        \"\"\"\n",
    "            TODO: write generation code here\n",
    "\n",
    "            Generate a sequence of words given a starting sequence.\n",
    "            :param inp: Initial sequence of words (batch size, length)\n",
    "            :param forward: number of additional words to generate\n",
    "            :return: generated words (batch size, forward)\n",
    "        \"\"\"        \n",
    "        raise NotImplemented\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HCVG5YISwRW"
   },
   "outputs": [],
   "source": [
    "run_id = str(int(time.time()))\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbHH6zXTSwRa"
   },
   "outputs": [],
   "source": [
    "model = Model(len(vocab), embedding_dim=EMB_DIM, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "loader = DataLoaderForLanguageModeling(\n",
    "    dataset=dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    loader=loader, \n",
    "    max_epochs=NUM_EPOCHS, \n",
    "    run_id=run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D8wTJkBSwRc"
   },
   "outputs": [],
   "source": [
    "best_nll = 1e30 \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()\n",
    "    nll = trainer.test()\n",
    "    if nll < best_nll:\n",
    "        best_nll = nll\n",
    "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
    "        trainer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2FmDqBCSwRf"
   },
   "outputs": [],
   "source": [
    "# Don't change these\n",
    "# plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
    "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('NLL')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipdbmqaGSwRh"
   },
   "outputs": [],
   "source": [
    "# see generated output\n",
    "print (trainer.generated[-1]) # get last generated output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
